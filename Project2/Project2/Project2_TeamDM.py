# -*- coding: utf-8 -*-
"""Project2_ML_DAMIANS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_TfnmlfIrV1azmBAmP1yaqeTBjm0QQDy

# Project 2 - Naive Bayes and Logistic Regression
### Damian Franco
### Meiling Traeger

This project demonstrates a various machine learning models for text classification through linear regression and naive bayes models. There are three main sections (Reading in data sets, naive bayes and logistic regression) with many cells in each session, but there is some order within the chaos below.
"""

import numpy as np
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D 
import seaborn as sbs
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.patches import Circle
from google.colab import files
import os 
import sklearn as ski
import math
import scipy.stats as stats
from pprint import pprint
import dask
import dask.dataframe as dd
import dask.array as da
import dask.bag as db
import pickle
import csv
from collections import defaultdict
import json
import joblib

# Comment in if wanting to load data directly from Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""# Create and load data

## References:
* https://towardsdatascience.com/working-with-sparse-data-sets-in-pandas-and-sklearn-d26c1cfbe067
* https://www.analyticsvidhya.com/blog/2022/10/dealing-with-sparse-datasets-in-machine-learning/
* https://www.projectpro.io/recipes/reduce-dimentionality-on-sparse-matrix-in-python
* https://www.machinelearningplus.com/python/how-to-deal-with-big-data-in-python/
* https://www.dask.org/
* https://www.machinelearningplus.com/python/dask-tutorial/
* https://towardsdatascience.com/loading-large-datasets-in-pandas-11bdddd36f7b

### Loading in vocab list
"""

all_vocab = []
with open("drive/My Drive/machineLearning/newsText/vocabulary.txt", "r") as vocab_list:
	vocab_lines = vocab_list.readlines()

for l in vocab_lines:
  asvocab_list = l.split("\n")
  all_vocab.append(asvocab_list[0])

print(all_vocab)

print(len(all_vocab))

"""### Loading in group labels set"""

all_grouplabels = pd.read_csv('drive/My Drive/machineLearning/newsText/newsgrouplabels.txt', sep=" ", header=None)

all_grouplabels.head()

print(all_grouplabels.shape[0])

"""### Loading in training classes"""

train_classes = []
with open("drive/My Drive/machineLearning/newsText/all_classesTRAIN.txt", "r") as classes_list:
	classes_lines = classes_list.readlines()

for l in classes_lines:
  asclasses_list = l.split("\n")
  train_classes.append(int(asclasses_list[0]))

print(train_classes)

train_classes = np.array(train_classes)

train_classes

"""### Load "small" training set"""

df_trainSMALL = pd.read_csv("drive/My Drive/machineLearning/newsText/trainingSMALL.csv")

df_trainSMALL.head()

df_trainSMALL['14'].head()

df_trainSMALL['14'].value_counts()

"""### Load "small" testing set"""

df_testSMALL = pd.read_csv("drive/My Drive/machineLearning/newsText/testingSMALL.csv")

df_testSMALL.head()

"""### Load in actual training set"""

# Define the chunk size
chunk_size = 100

# Create an empty list to store the chunks
chunks = []

# Open the file in read mode
with open('drive/My Drive/machineLearning/newsText/training.csv', 'r') as f:

    # Create a Pandas TextFileReader object
    reader = pd.read_csv(f, chunksize=chunk_size)

    # Loop over each chunk
    for chunk in reader:
        # Process the chunk as needed
        # ...
        # Append the chunk to the list of chunks
        chunks.append(chunk)

# Concatenate the chunks into a single DataFrame
df_trainBIG = pd.concat(chunks, ignore_index=True)

df_trainBIG.shape

df_trainBIG.head(12000)

# Define the chunk size
chunk_size = 50

# Create an empty list to store the chunks
chunks = []

# Open the file in read mode
with open('drive/My Drive/machineLearning/newsText/testing.csv', 'r') as f:

    # Create a Pandas TextFileReader object
    reader = pd.read_csv(f, chunksize=chunk_size)

    # Loop over each chunk
    for chunk in reader:
        # Process the chunk as needed
        # ...
        # Append the chunk to the list of chunks
        chunks.append(chunk)

# Concatenate the chunks into a single DataFrame
df_testBIG = pd.concat(chunks, ignore_index=True)

df_testBIG.shape

df_testBIG.head(6773)

"""### Load full train set"""

df_trainFULL = pd.read_csv("drive/My Drive/machineLearning/newsText/trainingHALF.csv")

df_trainFULL.head()

"""## Load full test set"""

df_testFULL = pd.read_csv("drive/My Drive/machineLearning/newsText/testing.csv")

df_testFULL.head()

"""## Load validation set"""

df_validation = pd.read_csv("drive/My Drive/machineLearning/newsText/validation.csv")

df_testFULL.head()

"""## Dataframe to numpy"""

df_trainI = df_trainBIG.copy()

df_trainI = df_trainI.drop(columns=['1', '14'])

df_testI = df_testBIG.copy()

df_testI = df_testI.drop(columns=['12001'])

nump_trainBIG = df_trainI.to_numpy()

nump_testBIG = df_testI.to_numpy()

nump_trainBIG

nump_testBIG

len(nump_trainBIG)

len(nump_testBIG)

all_classesBIG = df_trainBIG['14'].to_numpy()

all_classesBIG

print(all_classesBIG)
print(len(all_classesBIG))

"""## Chunking Attempt"""

# # CSV TO .NPY

# # Set the chunk size
# chunk_size = 500

# # Open the CSV file using pandas' read_csv function
# csv_file = pd.read_csv('drive/My Drive/machineLearning/newsText/testing.csv', iterator=True, chunksize=chunk_size)

# # Loop over the chunks of the CSV file
# for i, chunk in enumerate(csv_file):
#     # Convert the chunk to a NumPy array
#     np_array = chunk.values
    
#     # Save the NumPy array to a .npy file
#     np.save(f'chunk_{i}.npy', np_array)

#     # Download the .npy file to your local machine
#     files.download(f'chunk_{i}.npy')

# Set the chunksize parameter to control the size of each chunk
# chunksize = 100000

# # Open the CSV file and read it in chunks
# for i, chunk in enumerate(pd.read_csv('your_csv_file.csv', chunksize=chunksize)):
#     # Convert the chunk to a pickle file
#     with open(f'your_pickle_file_{i}.pkl', 'wb') as f:
#         pickle.dump(chunk, f)

# READ IN .NPY

# Set the number of chunks
num_chunks = 23

# Initialize an empty list to hold the chunks
chunks = []

# Loop over the chunks and load them into memory
for i in range(num_chunks):
    # Load the chunk from the .npy file
    chunk = np.load(f'drive/My Drive/machineLearning/newsText/npyTrainChunks/chunk_{i}.npy')
    
    # Append the chunk to the list of chunks
    chunks.append(chunk)

# Concatenate the chunks together to get the full dataset
np_trainstep = np.concatenate(chunks)

np_trainlist = []
for ro in np_trainstep:
  ro = ro[1:-1]
  np_trainlist.append(ro)

np_trainBIG = np.array(np_trainlist)

np_trainBIG

# READ IN .NPY

# Set the number of chunks
num_chunks = 13

# Initialize an empty list to hold the chunks
chunks = []

# Loop over the chunks and load them into memory
for i in range(num_chunks):
    # Load the chunk from the .npy file
    chunk = np.load(f'drive/My Drive/machineLearning/newsText/npyTestChunks/chunk_{i}.npy')
    
    # Append the chunk to the list of chunks
    chunks.append(chunk)

# Concatenate the chunks together to get the full dataset
np_teststep = np.concatenate(chunks)

np_testlist = []
for ro in np_teststep:
  ro = ro[1:]
  np_testlist.append(ro)

np_testBIG = np.array(np_testlist)

np_testBIG

"""# Model 2: Naive Bayes Classifier

## References:
* https://towardsdatascience.com/gaussian-naive-bayes-4d2895d139a
* https://www.geeksforgeeks.org/naive-bayes-classifiers/
* https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/
* https://medium.com/@rangavamsi5/na%C3%AFve-bayes-algorithm-implementation-from-scratch-in-python-7b2cc39268b9
* https://www.datacamp.com/tutorial/naive-bayes-scikit-learn
* https://www.kaggle.com/code/prashant111/naive-bayes-classifier-in-python
* https://www.analyticsvidhya.com/blog/2021/09/naive-bayes-algorithm-a-complete-guide-for-data-science-enthusiasts/
* https://towardsdatascience.com/bayes-classifier-with-maximum-likelihood-estimation-4b754b641488
* https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/
* https://stats.stackexchange.com/questions/497507/mle-and-map-with-naive-bayes

## Maximum Likelihood Estimation (MLE)
"""

# Maximum Likelihood function to estimation P(Y_k)
def MLE(train_data, alpha=1.0):
    class_counts = defaultdict(int)
    counts = defaultdict(lambda: defaultdict(int))

    for features, label in train_data:
        class_counts[label] += 1
        for feature in features:
            counts[label][feature] += 1

    vocab = set(feature for features, label in train_data for feature in features)
    log_likelihood = {label: {feature: np.log((counts[label][feature] + alpha) / (sum(counts[label].values()) + alpha * len(vocab)))
                              for feature in vocab} for label in class_counts}

    return log_likelihood

testMLE = MLE(10, df_trainSMALL, '14')
print(testMLE)

# Get P(Y) for all documents
allMLE_probs = []

for i in range(1, 21):
  curr_MLEest = MLE(i, df_trainSMALL, '14')
  allMLE_probs.append(curr_MLEest)
print(allMLE_probs)

"""## Maximum a Posteriori Probability (MAP)"""

# Maximum a Posteriori function to estimation P(X_i | Y_k)
def MAP(train_data, alpha=1.0, prior_weight=1.0):
    class_counts = defaultdict(int)
    counts = defaultdict(lambda: defaultdict(int))

    for features, label in train_data:
        class_counts[label] += 1
        for feature in features:
            counts[label][feature] += 1

    log_prior = {label: np.log((count + alpha * prior_weight) / (len(train_data) + alpha))
                 for label, count in class_counts.items()}
    vocab = set(feature for features, label in train_data for feature in features)
    log_likelihood = {label: {feature: np.log((counts[label][feature] + alpha) / (sum(counts[label].values()) + alpha * len(vocab)))
                              for feature in vocab} for label in class_counts}

    return log_prior, log_likelihood

def create_vocab_dict(keys, values):
  curr_dict = {}
  for key in curr_keys:
    for value in curr_vals:
      curr_dict[key] = value
      curr_vals.remove(value)
      break
  return curr_dict;

# Make vocab list a dictionary
# Keys
curr_keys = list(df_trainSMALL.columns)
curr_keys.remove('1')
curr_keys.remove('14')
print(curr_keys)
print(len(curr_keys))

# Values
curr_vals = all_vocab
print(curr_vals)
print(len(curr_vals))

swapped_vocab_dict = create_vocab_dict(curr_keys, curr_vals)
print(swapped_vocab_dict)

vocab_dict = dict([(value, key) for key, value in swapped_vocab_dict.items()])
print(vocab_dict)

testing = df_trainSMALL.iloc[1]

testMAP = MAP(df_trainSMALL.iloc[1], df_trainSMALL, '14')
print(testMAP)

testMAPsum = np.sum(testMAP)
print(testMAPsum)

"""## Classify"""

def NB_classify(curr_instance, x_new, df_local):
  # curr_instance = df_local.iloc[i]
  curr_class = curr_instance['14']
  curr_MLE = MLE(curr_class, df_local, '14')
  curr_MAP = MAP(df_local.iloc[i], df_local, '14')
  prob_y = np.log2(curr_MLE)
  prob_xy = np.log2(curr_MAP)

  # x_new is the new instance we want to classify
  sum_new = prob_y + np.sum(x_new * prob_xy)
  # print(sum_new)
  return sum_new

all_calcs = []
for i in range(9):
  class_pred = NB_classify(df_trainSMALL.iloc[i], 1, df_trainSMALL)
  all_calcs.append(class_pred)

classifyTest = np.argmax(all_calcs)
print(classifyTest)

def predict(features, log_prior, log_likelihood):
    scores = {label: log_prior[label] for label in log_prior}
    for feature in features:
        for label in log_likelihood:
            if feature in log_likelihood[label]:
                scores[label] += log_likelihood[label][feature]
    return max(scores, key=scores.get)

"""### Practice run"""

data = [
    (['quick', 'brown', 'fox'], 'animal'),
    (['lazy', 'dog'], 'animal'),
    (['ate', 'food'], 'human'),
    (['person', 'ate'], 'human')
]

train_data = [(['hello', 'world'], 'greeting'),
              (['cat', 'dog'], 'animal'),
              (['red', 'green', 'blue'], 'color'),
              (['yes', 'no'], 'answer')]

log_prior, log_likelihood = train_mle(train_data)

# Test the model on a new instance
test_instance = ['hello', 'cat', 'red', 'yes']
scores = {label: log_prior[label] + sum(log_likelihood[label].get(feature, 0) for feature in test_instance)
          for label in log_prior}

# Print the predicted label
predicted_label = max(scores, key=scores.get)
print(predicted_label) # Output: 'greeting'

print(data[0][1])

"""### Train and test """

df_numpyset = df_trainFULL

df_numpyset.head()

class_list =  list(df_numpyset['14'])
print(class_list)

class_string = [str(x) for x in class_list]
type(class_string)
print(class_string)

df_validNUMP = df_validation
# df_validNUMP = df_validNUMP.drop('12001', axis=1)

valid_nump = df_validNUMP.to_numpy()

valid_nump = valid_nump.tolist()

valid_nump[0]

df_testNUMP = df_testFULL

# df_numpyset = df_numpyset.drop(columns=df_numpyset.columns[0], axis=1, inplace=True)
# df_numpyset = df_numpyset.drop('14', axis=1)
# df_testNUMP = df_testNUMP.drop('12001', axis=1)

df_testNUMP.head()

df_numpyset.head()

testing_nump = df_testNUMP.to_numpy()

testing_nump = testing_nump.tolist()

testing_nump[0]

type(testing_nump)

print(testing_nump[0])
print(len(testing_nump))

testing_np = list(zip(testing_nump, class_string))

print(testing_np[0])

log_prior_mle, log_likelihood_mle = train_mle(testing_np)

print(log_prior_mle)
print(log_likelihood_mle)

# Alpha and Beta terms
beta_t = 1 / len(all_vocab)
alpha_t = 1 + beta_t
log_prior_map, log_likelihood_map = train_map(testing_np)

print(log_prior_map)
print(log_likelihood_map)

import json

json_string = json.dumps(log_likelihood_map)

with open('my_dict.json', 'w') as f:
    f.write(json_string)

df_testFULL.head()

testing_np = df_testSMALL.to_numpy()

all_ids = df_testSMALL['12001'].to_numpy()

all_ids = np.insert(all_ids, 0, 12001, axis=0)

print(all_ids)

test_instanceNP = testing_np[0]
test_instance = np.delete(test_instanceNP, 0)
print(test_instance)

# Test the model on a new instance
# test_instance = [2, 3, 4]
scores = {label: log_prior_map[label] + sum(log_likelihood_map[label].get(feature, 0) for feature in test_instance)
          for label in log_prior_map}

# Print the predicted label
predicted_label = max(scores, key=scores.get)
print(predicted_label)

def NB_classify(testing_set, log_prior, log_likelihood):
  pred_classes = []
  for features in testing_set:
    feat = np.delete(features, 0)
    # print(feat)
    label = predict(feat, log_prior, log_likelihood)
    pred_classes.append(label)
    print('Prediction class:', label)
  return pred_classes

test_instanceNP = testing_np[0]
test_instance = np.delete(test_instanceNP, 0)
print(test_instance)

with open('/content/log_likelihood_map.json', 'r') as json_file:
    log_likelihood_map = json.load(json_file)
    print(log_likelihood_map)

for key in log_likelihood_map:
    if isinstance(log_likelihood_map[key], str) and log_likelihood_map[key].isdigit():
        log_likelihood_map[key] = int(log_likelihood_map[key])

print(log_likelihood_map)

for outer_key, inner_dict in log_likelihood_map.items():
    new_inner_dict = {}
    for string_key, value in inner_dict.items():
        integer_key = int(string_key)
        new_inner_dict[integer_key] = value
    log_likelihood_map[outer_key] = new_inner_dict

log_likelihood_map

print(log_likelihood_map['3'])

with open('/content/log_prior_map.json', 'r') as json_file:
    log_prior_map = json.load(json_file)
    print(log_prior_map)

for key in log_prior_map:
    if isinstance(log_prior_map[key], str) and log_prior_map[key].isdigit():
        log_prior_map[key] = int(log_prior_map[key])

print(log_prior_map)

predictions_MLE = NB_classify2(testing_nump, log_prior_map, log_likelihood_map)

predictions_MLE

with open('my_file.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['id', 'class'])  # write the header row
    for i, value in enumerate(predictions_MLE, start=12001):
        writer.writerow([i, value])

# csv_file = open('singleInstance.csv', 'r')
# csv_reader = csv.reader(csv_file)

# data_list = []

# for row in csv_reader:
#     int_row = [int(value) for value in row]
#     data_list.append(int_row)

# csv_file.close()

# data_list[0]

predictions_MAP = NB_classify2(testing_np, log_prior_map, log_likelihood_map)

print(predictions_MLE)

print(predictions_MAP)

"""## Validation"""

actual_valid =  list(df_validFULL['9'])

actual_valid

# Load the model from the file
nb = joblib.load('nb_model.pkl')

X_test = valid_nump

y_pred_nb = nb.predict(X_test)

print("Naive Bayes predicts:", y_pred_nb)

correct_nb = (y_pred_nb == actual_valid)
accuracy_nb = correct_nb.sum() / correct_nb.size

print(accuracy_nb)

"""## Plots"""

# create x and y data
x1 = [1, 2, 3, 4, 5, 6]
y1 = [9, 50, 78, 78, 70, 53]

# create x and y data for second line
x2 = [1, 2, 3, 4, 5, 6]
y2 = [55, 70, 82, 84, 84, 85]

# create plot
plt.plot(x1, y1, label='Validation', marker='o')
plt.plot(x2, y2, label='Testing', marker='o')

# set plot title and axis labels
plt.title('NB Accuracies')
plt.ylabel('Accuracy (%)')

# set axis limits
plt.xlim(1, 6)
plt.ylim(0, 100)
plt.xticks(x1, ['Model 1\nBeta = \n0.00001', 'Model 2\nBeta = \n0.0001', 'Model 3\nBeta = 0.001', 'Model 4\nBeta = 0.01', 'Model 5\nBeta = 0.1', 'Model 6\nBeta = 1'])

# add legend
plt.legend()
# plt.savefig('filename.png', dpi=300)

# show plot
plt.show()

# create x and y data
x1 = [1, 2, 3, 4]
y1 = [55, 75, 80, 45]

# create x and y data for second line
x2 = [1, 2, 3, 4]
y2 = [60, 80, 86, 86]

# create plot
plt.plot(x1, y1, label='Validation', marker='o')
plt.plot(x2, y2, label='Testing', marker='o')

# set plot title and axis labels
plt.title('LR Accuracies')
plt.ylabel('Accuracy (%)')

# set axis limits
plt.xlim(1, 4)
plt.ylim(0, 100)
plt.xticks(x1, ['Model 1\n eta/lambda = \n0.001', 'Model 2\n eta/lambda = \n0.01', 'Model 3\n eta/lambda = \n0.1', 'Model 4\n eta/lambda = \n1'])

# add legend
plt.legend()
# plt.savefig('filename.png', dpi=300)

# show plot
plt.show()



"""# Model 3: Logistic Regression Classifier

## References:
* https://realpython.com/logistic-regression-python/
* https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8
* https://www.w3schools.com/python/python_ml_logistic_regression.asp
* https://www.geeksforgeeks.org/ml-logistic-regression-using-python/
* https://www.analyticsvidhya.com/blog/2022/02/implementing-logistic-regression-from-scratch-using-python/
* https://www.udemy.com/course/data-science-logistic-regression-in-python/
* https://medium.com/codex/machine-learning-logistic-regression-with-python-5ed4ded9d146
* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Logistic%20Regression%20in%20Python%20-%20Step%20by%20Step.ipynb
* https://levelup.gitconnected.com/learn-logistic-regression-for-classification-with-python-10-practical-examples-f7f6119bebf9
* https://www.dataquest.io/blog/logistic-regression-in-python/

## Attempt 1
"""

from scipy.special import softmax
from sklearn.preprocessing import OneHotEncoder
onehot_encoder = OneHotEncoder()

def loss(X, Y, W):
    # print("In loss function")
    # print("X shape:", X.shape)
    # print("W shape:", W.shape)
    Z = - X.dot(W)
    N = X.shape[0]
    # onehot_encoder = OneHotEncoder()
    Y_onehot = onehot_encoder.fit_transform(Y.reshape(-1, 1))
    # print("Y_onehot shape:", Y_onehot.shape)
    loss = 1/N * (np.trace(X.dot(W).dot(Y_onehot.T)) + np.sum(np.log(np.sum(np.exp(Z), axis=1))))
    return loss

def gradient(X, Y_onehot, W, mu):
  # print("X shape:", X.shape)
  # print("Y_onehot shape:", Y_onehot.shape)
  # print("W shape:", W.shape)
  Z = X @ W
  P = softmax(Z, axis=1)
  # print("P shape:", P.shape)
  N = X.shape[0]
  Y_onehot = np.expand_dims(Y_onehot, axis=-1)
  gd = 1/N * (X.T @ (Y_onehot - P)) + 2 * mu * W
  # print("Gradient shape:", gd.shape)
  return gd

def gradient_descent(X, Y, max_iter=1000, eta=0.1, mu=0.01):
  Y_onehot = onehot_encoder.fit_transform(Y.reshape(-1,1))
  W = np.zeros((X.shape[1], Y_onehot.shape[1]))
  step = 0
  step_lst = []
  loss_lst = []
  W_lst = []

  while step < max_iter:
    step += 1
    W -= eta * gradient(X, Y, W, mu)
    step_lst.append(step)
    W_lst.append(W)
    loss_lst.append(loss(X, Y, W))

  df = pd.DataFrame({
      'step': step_lst,
      'loss': loss_lst
  })
  return df, W

class Multiclass:
  def fit(self, X, Y):
    self.loss_steps, self.W = gradient_descent(X, Y)
    
  def loss_plot(self):
    return self.loss_steps.plot(
        x='step',
        y='loss',
        xlabel='step',
        ylabel='loss'
    )

  def predict(self, H):
    Z = - H @ self.W
    P = softmax(Z, axis=1)
    return np.argmax(P, axis=1)

df_trainLOG = df_trainSMALL.copy()

df_trainLOG.head(1510)

df_trainLOG = df_trainLOG.drop(columns=['1', '14'])

df_trainLOG.head(1500)

train_nump = df_trainLOG.to_numpy()
print(train_nump[1498])

len(train_nump)

all_classes = df_trainSMALL['14'].unique()
print(all_classes)
print(len(all_classes))

all_trainClass = df_trainSMALL['14'].to_numpy()

print(all_trainClass)
print(len(all_trainClass))

print(all_trainClass[1498])
print(train_nump[1498])

print(len(all_trainClass))
print(len(train_nump))

X = np.random.rand(100, 5)
print(X)

Y = np.random.randint(0, 3, size=100)
print(Y)

print(train_nump)

logModel = Multiclass()
logModel.fit(X, Y)

X_test = np.random.rand(20, 5)

# Make predictions on test data X_test
predictions = logModel.predict(X_test)

# Print the predictions
print(predictions)

"""## Attempt 2"""

class LogReg:
    def __init__(self, learning_rate=0.01, num_iterations=10000):
        self.learning_rate = learning_rate
        self.num_iterations = num_iterations
        
    def fit(self, X, y):
        self.num_classes = np.max(y) + 1  # set num_classes based on actual number of classes
        self.num_features = X.shape[1]
        self.weights = np.zeros((self.num_classes, self.num_features))
        self.bias = np.zeros(self.num_classes)
        y_encoded = self.one_hot_encoding(y)

        for i in range(self.num_iterations):
            z = np.dot(X, self.weights.T) + self.bias
            y_pred = softmax(z)

            loss = cross_entropy_loss(y_encoded, y_pred)
            print("Iteration {}: Loss = {:.4f}".format(i, loss))

            error = y_pred - y_encoded
            grad_w = np.dot(error.T, X) / X.shape[0]
            grad_b = np.mean(error, axis=0)

            self.weights -= self.learning_rate * grad_w
            self.bias -= self.learning_rate * grad_b
    
    def predict(self, X):
        z = np.dot(X, self.weights.T) + self.bias
        y_pred = softmax(z)
        return np.argmax(y_pred, axis=1)
    
    def one_hot_encoding(self, y):
        y_encoded = np.zeros((len(y), self.num_classes))
        print(len(y))
        for i in range(len(y)):
            
            y_encoded[i, y[i]] = 1
        return y_encoded

    def cross_entropy_loss(y_encoded, y_pred):
        N = y_encoded.shape[0]
        loss = -np.sum(y_encoded * np.log(y_pred)) / N
        return loss

"""### Accuracy test run"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Testing out model with some simplier data sets
iris = load_iris()
X, y = iris.data, iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogReg()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

X_train

y_train

print(len(X_train))
print(len(y_train))

"""### Train and test model"""

df_trainLOG = df_trainSMALL.copy()

df_trainLOG = df_trainLOG.drop(columns=['1', '14'])

df_testLOG = df_testSMALL.copy()

df_testLOG = df_testLOG.drop(columns=['12001'])

train_nump = df_trainLOG.to_numpy()

test_nump = df_testLOG.to_numpy()

print(train_nump[1498])

train_nump

test_nump

len(train_nump)

all_classes = df_trainSMALL['14'].unique()
print(all_classes)
print(len(all_classes))

all_trainClass = df_trainSMALL['14'].to_numpy()

all_trainClass

print(all_trainClass)
print(len(all_trainClass))

print(all_trainClass[1498])
print(train_nump[1498])

print(len(all_trainClass))
print(len(train_nump))

np_model = LogReg()

np_model.fit(train_nump, all_trainClass)

np_model.weights

np_model.weights[0]

np.save('LRweights_4.npy', np_model.weights)
files.download('LRweights_4.npy')

# Use the model to make predictions on the testing set
y_pred = np_model.predict(test_nump)

print(len(test_nump))

print(y_pred)

"""## Validation"""

actual_valid =  list(df_validFULL['9'])

actual_valid

# Load the model from the file
nb = joblib.load('nb_model.pkl')

X_test = valid_nump

y_pred_nb = nb.predict(X_test)

print("Naive Bayes predicts:", y_pred_nb)

correct_nb = (y_pred_nb == actual_valid)
accuracy_nb = correct_nb.sum() / correct_nb.size

print(accuracy_nb)

"""# Most important features (mutual information)

## References:
* http://www.sefidian.com/2017/06/14/mutual-information-mi-and-entropy-implementations-in-python/
* https://towardsdatascience.com/select-features-for-machine-learning-model-with-mutual-information-534fe387d5c8
* https://stackoverflow.com/questions/20491028/optimal-way-to-compute-pairwise-mutual-information-using-numpy
"""

# Estimate H(X) for each instance
def entropy(p):
    p = p[np.nonzero(p)]  # remove zero probabilities
    H = -np.sum(p * np.log2(p))
    return H

# Estimate I(X) given then H(X) or instance and target class
def mutual_info(x, y, bins=10):
    hist_xy, _, _ = np.histogram2d(x, y, bins=bins)
    p_x = np.histogram(x, bins=bins)[0] / x.size
    p_y = np.histogram(y, bins=bins)[0] / y.size
    H_x = entropy(p_x)
    H_y = entropy(p_y)
    p_xy = hist_xy / np.sum(hist_xy)
    H_xy = entropy(p_xy)
    MI_xy = H_x + H_y - H_xy
    return MI_xy

mut_infoScore = mutual_info(train_nump, train_classes)
print(mut_infoScore)

sortedALL = ['adg',
 'amdahl',
 'alomar',
 'cryptanalysts',
 'mitteilungsblatt',
 'xfor',
 'adsp',
 'atheism',
 'athos',
 'sportstalk',
 'azerbaijanians',
 'tettleton',
 'memmedov',
 'motorcyclists',
 'bontchev',
 'bruins',
 'nba',
 'canadiens',
 'albicans',
 'cardinals',
 'deciphering',
 'christianity',
 'ttttttttttttttt',
 'countersteering',
 'cousineau',
 'powerusersgroupchairman',
 'cryptography',
 'cryptographic',
 'denning',
 'dineen',
 'srcsignature',
 'xdpyinfo',
 'autopsies',
 'firearm',
 'fprintf',
 'gregmeister',
 'god',
 'gfci',
 'goaltender',
 'goalie',
 'blackhawks',
 'escherichia',
 'suprafaxmodem',
 'homicides',
 'maciisi',
 'imakefile',
 'infante',
 'inning',
 'ioccc',
 'mmmmmmmmmm',
 'nordiques',
 'karabakh',
 'konfessionslosesn',
 'nctams',
 'leafs',
 'lemieux',
 'libxmu',
 'excalibur',
 'kovalev',
 'turgeon',
 'moncton',
 'mutants',
 'mydisplay',
 'nhl',
 'nsmca',
 'obfuscated',
 'obp',
 'oilers',
 'oname',
 'orbiter',
 'pitcher',
 'prometheus',
 'plaintext',
 'potvin',
 'powerbook',
 'punisher',
 'ranck',
 'rayshade',
 'rbi',
 'recchi',
 'ripem',
 'rlk',
 'rsa',
 'unverzagt',
 'sandberg',
 'sdpa',
 'serdar',
 'soderstrom',
 'ssto',
 'stderr',
 'stephanopoulos',
 'stepanakert',
 'uccxkvb',
 'zrepachol',
 'whalers',
 'wip',
 'jjrj',
 'zeitlin',
 'xfree',
 'acsddc']

sortedALL = sorted(sortedALL)

sortedALL

sorted_vocab = sorted(all_vocab)

sorted_vocab